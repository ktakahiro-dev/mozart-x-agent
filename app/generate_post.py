
import os, sys, re, json, base64, time, datetime
import pandas as pd
import tweepy
from PIL import Image
from openai import OpenAI

# ====== Config via env ======
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TEXT_MODEL = os.getenv("OPENAI_MODEL_TEXT", "gpt-4o-mini")
TEXT_MODEL_FALLBACK = os.getenv("OPENAI_MODEL_TEXT_FALLBACK", "gpt-4o")
IMAGE_MODEL = os.getenv("OPENAI_MODEL_IMAGE", "gpt-image-1")

X_API_KEY = os.getenv("X_API_KEY")
X_API_SECRET = os.getenv("X_API_SECRET")
X_ACCESS_TOKEN = os.getenv("X_ACCESS_TOKEN")
X_ACCESS_SECRET = os.getenv("X_ACCESS_SECRET")
X_BEARER_TOKEN = os.getenv("X_BEARER_TOKEN")

CATALOG_CSV = os.getenv("CATALOG_CSV", "data/mozart_catalog.csv")
OUT_DIR = os.getenv("OUT_DIR", "out")
IMG_FILENAME = os.getenv("IMG_FILENAME", "mozart_post.png")
MAX_TRIES = int(os.getenv("OPENAI_MAX_TRIES", "3"))

# final output 16:9 for X
TARGET_W, TARGET_H = 1600, 900
# allowed by gpt-image-1 (landscape)
GEN_SIZE = "1536x1024"

os.makedirs(OUT_DIR, exist_ok=True)

# ---- helpers ----
EMOJI_RE = re.compile(r"[\U0001F300-\U0001FAFF\U00002700-\U000027BF]")  # broad emoji range
HASHTAG_RE = re.compile(r"#\S+")

def choose_piece(today: datetime.date, path: str):
    df = pd.read_csv(path)
    row = df.iloc[today.toordinal() % len(df)]
    return row.to_dict()

def clamp(s: str, max_len: int):
    s = (s or "").strip()
    return s if len(s) <= max_len else s[:max_len-1] + "‚Ä¶"

def strip_hashtags(s: str) -> str:
    s = HASHTAG_RE.sub("", s or "")
    s = re.sub(r"\s{2,}", " ", s).strip()
    return s

def ensure_one_emoji(s: str) -> str:
    if not EMOJI_RE.search(s or ""):
        # add a musical emoji if none present
        cand = "üéµ"
        s2 = (s + " " + cand).strip()
        return clamp(s2, 120)
    return s

def extract_json(text: str) -> str:
    m = re.search(r'\{.*\}', text, re.S)
    if m:
        return m.group(0)
    m2 = re.search(r'```(?:json)?\s*(\{.*\})\s*```', text, re.S|re.I)
    if m2:
        return m2.group(1)
    return text.strip()

def prompt_text(piece: dict) -> str:
    return f"""Êó•Êú¨Ë™û„Åß„É¢„Éº„ÉÑ„Ç°„É´„Éà‰ΩúÂìÅ„ÅÆXÊäïÁ®øÊñá„ÇíJSON„ÅßËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇJSON‰ª•Â§ñ„ÅØ‰∏ÄÂàáÊõ∏„Åã„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
{{
  "tweet": "<ÂÖ®ËßíËæº„Åø120Â≠ó‰ª•ÂÜÖ„ÄÇÁµµÊñáÂ≠ó„Çí1„Å§ÂÖ•„Çå„Çã„ÄÇ„Éè„ÉÉ„Ç∑„É•„Çø„Ç∞„ÅØÂÖ•„Çå„Å™„ÅÑÔºàË®òÂè∑#„Çí‰Ωø„Çè„Å™„ÅÑÔºâ>",
  "alt": "<ÁîªÂÉè„ÅÆ‰ª£Êõø„ÉÜ„Ç≠„Çπ„Éà„ÄÇ80-120Â≠ó„ÄÇÁµµÊñáÂ≠ó„Å®„Éè„ÉÉ„Ç∑„É•„Çø„Ç∞„ÅØÂÖ•„Çå„Å™„ÅÑ>",
  "img_caption": "<ÁîªÂÉè„Å´ÂÖ•„Çå„ÇãÁü≠„ÅÑË¶ãÂá∫„ÅóÔºà8-12Â≠óÔºâ„ÄÇÁµµÊñáÂ≠ó„Å®„Éè„ÉÉ„Ç∑„É•„Çø„Ç∞„ÅØÂÖ•„Çå„Å™„ÅÑ>"
}}
ÂØæË±°‰ΩúÂìÅ: {piece["ja_title"]}Ôºà{piece["en_title"]}Ôºâ / {piece["k"]}
Âè£Ë™ø: Ê∏©„Åã„ÅèÁ∞°ÊΩî„ÄÇÂ∞ÇÈñÄÁî®Ë™û„ÇíÈÅø„Åë„Çã„ÄÇ
"""

def prompt_image(piece: dict, caption: str) -> str:
    return f"""‰∏äÂìÅ„ÅßËêΩ„Å°ÁùÄ„ÅÑ„ÅüÈü≥Ê•Ω„Éì„Ç∏„É•„Ç¢„É´„ÄÇËÉåÊôØ„ÅØÊüî„Çâ„Åã„Å™Á¥ôË≥™„ÄÅË≠úÈù¢„Éª‰∫îÁ∑ö„ÉªÊ•ΩÂô®„Ç∑„É´„Ç®„ÉÉ„Éà„ÇíÊäΩË±°ÁöÑ„Å´„ÄÇ
‰∏≠Â§Æ„Å´„Äå{caption}„Äç„Å†„Åë„ÇíÊòéÁû≠„Å´ÈÖçÁΩÆ„ÄÇËâ≤Ë™ø„ÅØÁôΩ„ÄúÁîüÊàê„Éª„Ç∞„É¨„ÉºÂü∫Ë™ø„Å´Èáë„ÅÆ„Ç¢„ÇØ„Çª„É≥„Éà„ÄÇ‰ΩôÁôΩÂ§ö„ÇÅ„ÅßÂèØË™≠ÊÄßÈáçË¶ñ„ÄÇÈáçË¶ÅË¶ÅÁ¥†„ÅØ‰∏≠Â§ÆÂØÑ„Åõ„ÄÇ‰∏ä‰∏ã„Å´ÂçÅÂàÜ„Å™‰ΩôÁôΩ„ÄÇ"""

def call_chat(client: OpenAI, model: str, prompt: str) -> str:
    rsp = client.chat.completions.create(
        model=model,
        messages=[{"role":"user","content":prompt}],
        temperature=0.7
    )
    return rsp.choices[0].message.content or ""

def gen_text_alt_caption(client: OpenAI, piece: dict):
    last_error = None
    for model in [TEXT_MODEL, TEXT_MODEL_FALLBACK]:
        for attempt in range(1, MAX_TRIES+1):
            try:
                raw = call_chat(client, model, prompt_text(piece))
                blob = extract_json(raw)
                data = json.loads(blob)
                tweet = clamp(strip_hashtags(data.get("tweet","")), 120)
                alt = clamp(strip_hashtags(data.get("alt","")), 120)  # alt: no emoji or hashtags encouraged
                caption = clamp(strip_hashtags(data.get("img_caption", piece["ja_title"])), 12)
                # enforce at least one emoji in tweet
                tweet = ensure_one_emoji(tweet)
                print(f"[INFO] used_model={model}, attempt={attempt}, json_ok=True")
                return tweet, alt, caption
            except Exception as e:
                last_error = e
                print(f"[WARN] JSON parse failed (model={model}, attempt={attempt}): {e}")
                time.sleep(1.2 * attempt)

    # FallbackÔºàtweet„Å´1ÁµµÊñáÂ≠ó„ÄÅalt/caption„ÅØÁµµÊñáÂ≠ó„Å™„ÅóÔºâ
    tweet = ensure_one_emoji(clamp(f"{piece['ja_title']}„ÄÇÊúù„ÅÆ„Å≤„Å®ÊôÇ„Å´„Å©„ÅÜ„Åû„ÄÇ", 120))
    alt = clamp(f"{piece['ja_title']}Ôºà{piece['en_title']}Ôºâ„ÄÇ„É¢„Éº„ÉÑ„Ç°„É´„Éà„ÅÆÈ≠ÖÂäõ„Çí„ÇÑ„Åï„Åó„Åè‰ºù„Åà„Çã„Ç§„É°„Éº„Ç∏„ÄÇ", 120)
    caption = clamp(piece['ja_title'], 12)
    print(f"[INFO] used_model=fallback_template, json_ok=False")
    return tweet, alt, caption

def gen_image_and_fit(client: OpenAI, piece: dict, caption: str, out_path: str):
    # Generate at allowed size
    img = client.images.generate(model=IMAGE_MODEL, prompt=prompt_image(piece, caption), size=GEN_SIZE)
    b64 = img.data[0].b64_json
    with open(out_path, "wb") as f:
        f.write(base64.b64decode(b64))

    # Center-crop to 16:9 then resize to 1600x900
    im = Image.open(out_path)
    w, h = im.size  # expect 1536x1024
    target_ratio = TARGET_W / TARGET_H  # 1.777...
    new_h = int(round(w / target_ratio))  # 1536 -> 864
    if new_h <= h:
        top = (h - new_h) // 2  # 1024-864=160 -> 80px top/bottom
        im = im.crop((0, top, w, top + new_h))
    else:
        new_w = int(round(h * target_ratio))
        left = (w - new_w) // 2
        im = im.crop((left, 0, left + new_w, h))
    im = im.resize((TARGET_W, TARGET_H), Image.LANCZOS)
    im.save(out_path)

def post_to_x(text: str, image_path: str, alt_text: str):
    # v2 client for tweet
    client_v2 = tweepy.Client(
        consumer_key=X_API_KEY,
        consumer_secret=X_API_SECRET,
        access_token=X_ACCESS_TOKEN,
        access_token_secret=X_ACCESS_SECRET,
        bearer_token=X_BEARER_TOKEN
    )

    media_ids = None
    try:
        # v1.1: media upload + ALT
        auth = tweepy.OAuth1UserHandler(X_API_KEY, X_API_SECRET, X_ACCESS_TOKEN, X_ACCESS_SECRET)
        api_v1 = tweepy.API(auth)
        media = api_v1.media_upload(filename=image_path)
        api_v1.create_media_metadata(media_id=media.media_id, alt_text=alt_text)
        media_ids = [media.media_id]
    except tweepy.errors.BadRequest as e:
        print("[WARN] media upload BadRequest. Posting text-only.", e)
    except tweepy.errors.Forbidden as e:
        print("[WARN] media upload Forbidden. Posting text-only.", e)

    if media_ids:
        return client_v2.create_tweet(text=text, media_ids=media_ids)
    else:
        return client_v2.create_tweet(text=text)

def main():
    if not OPENAI_API_KEY:
        print("‚ùå OPENAI_API_KEY is not set.", file=sys.stderr)
        sys.exit(1)

    client = OpenAI(api_key=OPENAI_API_KEY)

    today = datetime.date.today()
    piece = choose_piece(today, CATALOG_CSV)

    tweet, alt, caption = gen_text_alt_caption(client, piece)
    print("[OUT] tweet:", tweet)
    print("[OUT] alt  :", alt)
    print("[OUT] caption:", caption)

    out_img = os.path.join(OUT_DIR, IMG_FILENAME)
    gen_image_and_fit(client, piece, caption, out_img)
    print("[OUT] image saved:", out_img)

    if all([X_API_KEY, X_API_SECRET, X_ACCESS_TOKEN, X_ACCESS_SECRET, X_BEARER_TOKEN]):
        resp = post_to_x(tweet, out_img, alt)
        print("[OK] tweeted:", resp.data)
    else:
        print("[SKIP] X credentials missing; tweet not posted.")

if __name__ == "__main__":
    main()
